{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sommaire \n",
    "1. [Sommaire](#ch1) <a class=\"anchor\" id=\"ch1\"></a>\n",
    "1. [Contexte Python](#ch2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d632b08c-d252-4238-b496-e2c6edebec4b",
    "_uuid": "eb13bf76d4e1e60d0703856ec391cdc2c5bdf1fb"
   },
   "source": [
    "<a class=\"anchor\" id=\"ch2\"></a>\n",
    "## [Contexte Python](#ch1) \n",
    "[Chapitre préc.](#ch1) - [Chapitre suiv.](#ch3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d632b08c-d252-4238-b496-e2c6edebec4b",
    "_uuid": "eb13bf76d4e1e60d0703856ec391cdc2c5bdf1fb"
   },
   "source": [
    "### Import des librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### builtin :\n",
    "\n",
    "# System\n",
    "import os, sys, gc, psutil \n",
    "\n",
    "# Gestion des dates\n",
    "from datetime import datetime, date, time, timedelta, timezone\n",
    "from dateutil import tz\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import math\n",
    "import random, re\n",
    "#import random, re, psutil, string, math, tkinter\n",
    "#from os import listdir\n",
    "from collections import Counter\n",
    "#from glob import glob\n",
    "from itertools import compress\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début des traitements à 05/09/2023, 12:20:14\n"
     ]
    }
   ],
   "source": [
    "startime = datetime.now()\n",
    "print(f'Début des traitements à {startime.strftime(\"%d/%m/%Y, %H:%M:%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#### for notebook process :\n",
    "import dummy as dummy # dummy chargé pour vérifier que le scrapping de la version sur pypi.org fonctionne sur le projet data-dummy\n",
    "\n",
    "import IPython as ipy\n",
    "from IPython.display import display, Markdown, Latex, HTML\n",
    "\n",
    "import ipywidgets as ipw\n",
    "from ipywidgets import FloatProgress  # affichage de la barre de progession des traitements\n",
    "\n",
    "\n",
    "#### numpy and pandas for data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import scipy as sci\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "import featuretools as ft\n",
    "import woodwork.logical_types as ft_lt\n",
    "from featuretools import selection\n",
    "#from featuretools.variable_types import list_variable_types\n",
    "\n",
    "\n",
    "#### machine learning :\n",
    "import sklearn as skl  # Pour afficher la version du package\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold, RFECV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import KFold, cross_validate, cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, fbeta_score, adjusted_rand_score, accuracy_score, auc, roc_auc_score, roc_curve, make_scorer, make_scorer, classification_report, fbeta_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "#### For plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "plt.close(\"all\")\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import missingno as msno  # Affichage des données vides des fichiers\n",
    "\n",
    "\n",
    "#### Scrapping :\n",
    "import requests as req  # Pour afficher la version du package\n",
    "from requests import get  # On importe la fonction 'get' (téléchargement) de 'requests'\n",
    "\n",
    "import lxml as lxml  # Pour afficher la version du package\n",
    "from lxml import html\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "import shap\n",
    "\n",
    "#try:\n",
    "#except Exception as e: pass\n",
    "#raise Exception()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définitions des méthodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Description des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération d'une liste triée de valeurs unique sur une colonne d'un df\n",
    "def getDistinctValuesOfColonne(df, colonne, sort=False):\n",
    "    my_dict = dict.fromkeys(df[colonne].values)\n",
    "    dict_colonne = {k: v for k, v in my_dict.items() if pd.isna(k) == False}\n",
    "    if sort:\n",
    "        return list(sorted(dict_colonne.keys()))\n",
    "    else:\n",
    "        return list(dict_colonne.keys())\n",
    "\n",
    "\n",
    "# Récupération du nombre d'outlier\n",
    "def fGetNombreOutliers(colonne_values):\n",
    "    # Check for outliers (assuming a normal distribution)\n",
    "    if colonne_values.dtypes in ['int64', 'float64']:\n",
    "        mean = colonne_values.mean()\n",
    "        std = colonne_values.std()\n",
    "        return ((colonne_values - mean).abs() > 3 * std).sum()\n",
    "    else:\n",
    "        return 'N/A'    \n",
    "\n",
    "\n",
    "# Récupération de données indormatives sur une colonne d'un df\n",
    "def getInfoOfOneDataFrameColonne(df, colonne, max_char=100, log=False):\n",
    "    list_colonne_value = list(set(df.loc[~df[colonne].isna(), colonne].unique()))\n",
    "    if log:\n",
    "        print(df[colonne].dtypes)\n",
    "        print(df[colonne].isnull().sum() / df.shape[0])\n",
    "        print(df[colonne].isnull().sum())\n",
    "        print(df[colonne].count())\n",
    "        print(len(list_colonne_value) / df.shape[0])\n",
    "        print(len(list_colonne_value))\n",
    "        print(fGetNombreOutliers(df.loc[~df[colonne].isna(), colonne]))\n",
    "        print('|'.join([str(_) for _ in list_colonne_value])[0:max_char:1])\n",
    "    return pd.DataFrame(data={\n",
    "        'Colonne':\n",
    "        colonne,\n",
    "        'type':\n",
    "        df[colonne].dtypes,\n",
    "        '% Null':\n",
    "        df[colonne].isnull().sum() / df.shape[0],\n",
    "        'Nb valeurs Null':\n",
    "        df[colonne].isnull().sum(),\n",
    "        'Nb valeurs':\n",
    "        df[colonne].count(),\n",
    "        '% Distinctes':\n",
    "        len(list_colonne_value) / df.shape[0],\n",
    "        'Nb valeurs distinctes':\n",
    "        len(list_colonne_value),\n",
    "        'Nombre d\\'outliers':\n",
    "        fGetNombreOutliers(df.loc[~df[colonne].isna(), colonne]),\n",
    "        'Liste valeurs':\n",
    "        '|'.join([str(_) for _ in list_colonne_value])[0:max_char:1]\n",
    "    }, index=[0])\n",
    "\n",
    "\n",
    "# Soulignement du texte\n",
    "def fSouligneTitle(title, decalage = 0):\n",
    "    souligne = ''\n",
    "    for i in range(decalage):\n",
    "        souligne = souligne + ' '\n",
    "    \n",
    "    len_title = max([len(x.strip()) for x in title.split(\"\\n\")])\n",
    "    for i in range(len_title):\n",
    "        souligne = souligne + '-'\n",
    "\n",
    "    return souligne\n",
    "\n",
    "\n",
    "# Imprime un texte souligné\n",
    "def fPrintTitleSouligne(title, decalage = 0):\n",
    "    line = ''\n",
    "    for i in range(decalage):\n",
    "        line = line + ' '\n",
    "\n",
    "    line = line + title\n",
    "    print(line)\n",
    "    print(fSouligneTitle(title, decalage))\n",
    "\n",
    "# Imprime un texte \n",
    "def fPrintTitle(title, decalage = 0, souligne=False):\n",
    "    line = ''\n",
    "    for i in range(decalage):\n",
    "        line = line + ' '\n",
    "\n",
    "    line = line + title\n",
    "    print(line)\n",
    "    if souligne:\n",
    "        print(fSouligneTitle(title, decalage))\n",
    "\n",
    "# Récupération de données indormatives d'un df\n",
    "def getInfoOfDataFrame(df, file, log=False):\n",
    "    \n",
    "    title_part = f'Description du fichier {file} :'\n",
    "    print(fSouligneTitle(title_part))\n",
    "    print(title_part)\n",
    "    print(fSouligneTitle(title_part))\n",
    "            \n",
    "    print()\n",
    "\n",
    "    # Dimension de DataFrame\n",
    "    print('=> Taille du DataFrame')\n",
    "    nb_lignes, nb_colonnes = df.shape\n",
    "    df_caracteristiques_fichier = pd.DataFrame(data={\n",
    "        'Nom du fichier': file,\n",
    "        'Colonnes': nb_colonnes,\n",
    "        'Lignes': nb_lignes\n",
    "    },\n",
    "                                               index=[0])\n",
    "    display(df_caracteristiques_fichier.set_index('Nom du fichier'))\n",
    "\n",
    "    if nb_lignes != 0 and nb_colonnes != 0:\n",
    "        # Création d'un DataFrame pour y stocker des informations des colonnes à afficher\n",
    "        df_detail_colonnes = pd.DataFrame()\n",
    "        if log: print(list(df))\n",
    "        for colonne in list(df):\n",
    "            if log: print(colonne)\n",
    "            df_detail_colonne_courant = getInfoOfOneDataFrameColonne(\n",
    "                df, colonne, log=log)\n",
    "            if log: print(df_detail_colonne_courant)\n",
    "            df_detail_colonnes = pd.concat(\n",
    "                [df_detail_colonnes, df_detail_colonne_courant],\n",
    "                ignore_index=True)    \n",
    "            if log: print(\"Passage colonne suivante.\\n\")\n",
    "            \n",
    "        pd.options.display.float_format = '{:.2%}'.format\n",
    "        print('=> Description des colonnes')\n",
    "        display(HTML(df_detail_colonnes.to_html()))\n",
    "        print()\n",
    "\n",
    "        print(f'=> Nombre de lignes en double : {df.duplicated().sum()}')\n",
    "        print()\n",
    "\n",
    "        # Représentation graphique du replisage du dataset\n",
    "        print('=> Représentation graphique du remplissage du jeu de données')\n",
    "        msno.matrix(df, color=(0, 100 / 255, 170 / 255))\n",
    "        plt.show()\n",
    "        print()\n",
    "        \n",
    "        # Représentation graphique # Affichage des valeurs manquantes en barplot\n",
    "        if df.isna().any().sum() > 0:\n",
    "            print('=> Représentation graphique des données manquantes')        \n",
    "            plot_null_prop(df)\n",
    "            print()\n",
    "\n",
    "        pd.options.display.float_format = '{:.2f}'.format\n",
    "        \n",
    "        df_types_num = [k for k, v in df.dtypes.to_dict().items() if str(v) != 'object']\n",
    "        if len(df_types_num) > 0:\n",
    "            print('=> Statistique sur les données numériques')\n",
    "            display(HTML(df[df_types_num].describe().T.to_html()))\n",
    "            print()\n",
    "\n",
    "        df_types_cat = [k for k, v in df.dtypes.to_dict().items() if str(v) == 'object']\n",
    "        if len(df_types_cat) > 0:\n",
    "            print('=> Statistique sur les données catégorielles')\n",
    "            display(HTML(df[df_types_cat].describe().T.to_html()))\n",
    "            print()\n",
    "            \n",
    "        print('=> Présentation succinctes des données')\n",
    "        display(df.head(5).T)\n",
    "        print()\n",
    "\n",
    "\n",
    "# Ajouter le label pour les valeurs \n",
    "def addlabels(x):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(x[i] + 0.01, i, f'{x[i]:.1%}', ha='left', va='center')\n",
    "\n",
    "\n",
    "# Affichage des valeurs manquantes en barplot\n",
    "def plot_null_prop(df, commentaire=\"\"):\n",
    "    \"\"\"\n",
    "    Input : dataframe, commentaire (option)\n",
    "    Output : graphique\n",
    "    Affiche dans un bar plot, le % de valeurs manquantes de chaque colonne du dataframe \n",
    "    \"\"\"\n",
    "    \n",
    "    # Proportion de valeurs manquantes par colonne\n",
    "    null_counts = df.isnull().mean(axis=0).sort_values(ascending=True)\n",
    "    \n",
    "    # barplot des valeurs manquantes par colonne\n",
    "    fig, ax = plt.subplots(figsize=(8, null_counts.shape[0] * 0.3))\n",
    "    plt.barh(np.arange(len(null_counts[null_counts != 0])),null_counts[null_counts != 0], color=(0, 100 / 255, 170 / 255))\n",
    "    plt.yticks(np.arange(len(null_counts[null_counts != 0])),null_counts[null_counts != 0].index,rotation=0, fontsize=10)\n",
    "    \n",
    "    addlabels(null_counts[null_counts != 0])\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(0.25))\n",
    "    ax.xaxis.set_minor_locator(ticker.MultipleLocator(0.05))\n",
    "    ax.xaxis.set_major_formatter(ticker.PercentFormatter(xmax=1))\n",
    "    \n",
    "    plt.axvline(x = 1, color = 'grey')\n",
    "    plt.axvline(x = 0.50, color = 'grey', linestyle=':')\n",
    "\n",
    "    sns.despine()\n",
    "    plt.margins(y=0)\n",
    "    plt.tight_layout()\n",
    "    plt.title('Proportion de données manquantes dans les colonnes concernées\\n'+commentaire)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fGetDescribeDf(df, nb_ligne=60):\n",
    "    \n",
    "    df_types_num = [k for k, v in df.dtypes.to_dict().items() if str(v) != 'object']\n",
    "    if len(df_types_num) > 0:\n",
    "        print('=> Statistique sur les données numériques')\n",
    "        if len(df_types_num) <= nb_ligne:\n",
    "            display(df[df_types_num].describe().T)\n",
    "        else:\n",
    "            for i in range(int(len(df_types_num) / nb_ligne)):\n",
    "                display(df[df_types_num].describe().T.head((i + 1) * nb_ligne).tail(nb_ligne))\n",
    "            display(df[df_types_num].describe().T.tail(len(df_types_num) % nb_ligne))\n",
    "        print()\n",
    "\n",
    "    df_types_cat = [k for k, v in df.dtypes.to_dict().items() if str(v) == 'object']\n",
    "    if len(df_types_cat) > 0:\n",
    "        print('=> Statistique sur les données catégorielles')\n",
    "        if len(df_types_cat) <= nb_ligne:\n",
    "            display(df[df_types_cat].describe().T)\n",
    "        else:\n",
    "            for i in range(int(len(df_types_cat) / nb_ligne)):\n",
    "                display(df[df_types_cat].describe().T.head((i + 1) * nb_ligne).tail(nb_ligne))\n",
    "            display(df[df_types_cat].describe().T.tail(len(df_types_num) % nb_ligne))\n",
    "        print()\n",
    "\n",
    "    print('=> Variables à NaN :')\n",
    "    df_nan = pd.DataFrame(df.isna().sum(), columns=['Valeurs à NaN'])\n",
    "    df_nan['% de NaN'] = 100 * df_nan['Valeurs à NaN'] / df.shape[0]\n",
    "\n",
    "    if df_nan.shape[0] <= nb_ligne:\n",
    "        display(df_nan)\n",
    "    else:\n",
    "        for i in range(int(df_nan.shape[0] / nb_ligne)):\n",
    "            display(df_nan.head((i + 1) * nb_ligne).tail(nb_ligne))\n",
    "        display(df_nan.tail(df_nan.shape[0] % nb_ligne))\n",
    "    print()\n",
    "    \n",
    "    print('=> Entête du DataFrame :')\n",
    "    display(df.head())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fPrintFullBigDf(df, nb_ligne=60):\n",
    "\n",
    "    if df.shape[0] <= nb_ligne:\n",
    "        display(df)\n",
    "    else:\n",
    "        for i in range(int(df.shape[0] / nb_ligne)):\n",
    "            display(df.head((i + 1) * nb_ligne).tail(nb_ligne))\n",
    "        display(df.tail(df.shape[0] % nb_ligne))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Récupération de la version courrante du package sur pypi.org\n",
    "def getCurrentVersionPipProject(project, uri='https://pypi.org/project'):\n",
    "    if project == 'sklearn':\n",
    "        project = 'scikit-learn'\n",
    "\n",
    "    if project == 'PIL':\n",
    "        project = 'pillow'\n",
    "        \n",
    "    if project == 'cv2':\n",
    "        project = 'opencv-python'\n",
    "\n",
    "    return scrapingDataFromHtml('/'.join(\n",
    "        [uri,\n",
    "         project]), '//h1[@class=\"package-header__name\"]/text()')[0].strip()\n",
    "\n",
    "\n",
    "# Scraping générique\n",
    "def scrapingDataFromHtml(url, xpath):\n",
    "    # uri : url racince de la page du projet à scraper\n",
    "    html_value = None\n",
    "    try:\n",
    "        # code that may cause exception\n",
    "        response = get(url)\n",
    "        if response.status_code == 200:\n",
    "            # Si la requete s'est bien passee\n",
    "            tree = html.fromstring(response.content)\n",
    "            # Get element using XPath\n",
    "            html_value = tree.xpath(xpath)\n",
    "        else:\n",
    "            html_value = [f' Page_non_trouvée_{url.split(\"/\")[-1]}']\n",
    "    except:\n",
    "        # code to run when exception occurs\n",
    "        html_value = [' Internet_non_disponible']\n",
    "\n",
    "    return html_value\n",
    "\n",
    "# Récupération de la version courrante de Python sur python.org\n",
    "def getCurrentVersionPython():\n",
    "    return scrapingDataFromHtml('https://www.python.org/', '//*[@id=\"content\"]/div/section/div[1]/div[2]/p[2]/a/text()')[0].split(' ')[1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Analyse de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fRepresentationDistributionVariableCategorielle(df, key, hue=None, hue_labels=None, max_values=60, log_scale=None, \n",
    "                                                    list_forcage_varaible_categorielle=None, table_name=None, dict_description_colonne=None):\n",
    "    list_variable_categorielle = [k for k, v in df.dtypes.to_dict().items() if ((str(v) == 'object') | (str(v) == 'bool'))]\n",
    "                                                        \n",
    "    if list_forcage_varaible_categorielle:\n",
    "        list_variable_categorielle = list_variable_categorielle + list_forcage_varaible_categorielle\n",
    "    for mesure in list(sorted(set(list_variable_categorielle))):\n",
    "        if len(list(set(df[mesure]))) != df.shape[0]:\n",
    "            fPrintTitleSouligne(mesure)\n",
    "            if ~((table_name == None) & (dict_description_colonne == None)):\n",
    "                print(dict_description_colonne.get(f\"{table_name} - {mesure}\"))\n",
    "\n",
    "            if (hue != None) & (mesure != hue):\n",
    "                df_4_plot = df[[key, hue, mesure]].copy().groupby(by=[mesure, hue]).count()\n",
    "                df_4_plot = pd.pivot_table(df_4_plot, values = key , index=mesure, columns=hue)\n",
    "                df_4_plot.rename(columns=hue_labels, inplace=True)\n",
    "                try:\n",
    "                    df_4_plot = df_4_plot.sort_values(by=df_4_plot.idxmax(axis=1)[0], ascending=True)\n",
    "                except:\n",
    "                    df_4_plot = df_4_plot.sort_values(by=list(df_4_plot)[0], ascending=True)\n",
    "                df_wip = df[[hue, mesure]]\n",
    "                df_wip[mesure] = df_wip[mesure].isna()\n",
    "                df_wip[hue] = df_wip[hue].apply(lambda x: hue_labels.get(x))\n",
    "                df_wip.columns = ['TARGET', 'NaN']\n",
    "                df_wip.set_index('TARGET', inplace=True)\n",
    "                df_wip = df_wip.groupby(by=hue).sum()\n",
    "\n",
    "            else:\n",
    "                df_4_plot = df[[key, mesure]].copy().groupby(by=mesure).count()\n",
    "                df_4_plot.rename(index=hue_labels, inplace=True)\n",
    "                df_4_plot = df_4_plot.sort_values(by=key, ascending=True)\n",
    "                df_wip = df[mesure].isna().sum()\n",
    "                df_wip = pd.DataFrame([[key, df_wip]], columns=['TARGET', 'NaN'])\n",
    "                df_wip.set_index('TARGET', inplace=True)\n",
    "\n",
    "            df_describe = df_4_plot.describe().T\n",
    "            colonnes = list(df_describe)\n",
    "            df_describe.reset_index(inplace=True)\n",
    "            df_describe.columns = ['TARGET'] + colonnes\n",
    "            df_describe = df_describe.set_index('TARGET')\n",
    "            df_describe = pd.concat([df_describe, df_wip], axis=1, ignore_index=True)\n",
    "            df_describe.columns = colonnes + ['NaN']\n",
    "            df_describe['NaN'] = df_describe.apply(lambda x: f\"{x['NaN']} [{100 * (x['NaN'] / x['max']):.2f}%]\", axis=1) \n",
    "            display(df_describe)\n",
    "            #display(df_4_plot)\n",
    "                \n",
    "            nb_values = df_4_plot.shape[0]\n",
    "    \n",
    "            #Réprésentation graphique de la distribution de la données\n",
    "            complement_titre = ''\n",
    "            size_h = nb_values\n",
    "            if max_values:\n",
    "                if max_values < nb_values:\n",
    "                    complement_titre = f\" [{max_values}/{nb_values} valeurs]\"\n",
    "                    size_h = max_values\n",
    "                    \n",
    "                df_4_plot = df_4_plot.head(max_values)\n",
    "    \n",
    "            if not log_scale:\n",
    "                log_scale = False\n",
    "                for col in list(df_4_plot):\n",
    "                    if (df_4_plot[col].min() / df_4_plot[col].max()) < 0.05:\n",
    "                        log_scale = True\n",
    "        \n",
    "            if (hue != None) & (mesure != hue):\n",
    "                ax = df_4_plot.plot.barh(color=['#2A7AB9', 'green', '#FF1A19'], log=log_scale, figsize=(10, size_h))\n",
    "                ax.bar_label(ax.containers[0])\n",
    "                ax.bar_label(ax.containers[1])\n",
    "                ax.bar_label(ax.containers[2])\n",
    "            else:\n",
    "                ax = df_4_plot.plot.barh(color='#2A7AB9', log=log_scale, figsize=(10, size_h))\n",
    "                ax.get_legend().remove()\n",
    "                ax.bar_label(ax.containers[0])\n",
    "            \n",
    "            plt.title(f\"Répartition des valeurs de {mesure}{complement_titre}\")\n",
    "            if log_scale:\n",
    "                plt.xlabel(\"Quantité (log)\")\n",
    "            else:\n",
    "                plt.xlabel(\"Quantité\")\n",
    "            \n",
    "            plt.ylabel(mesure)\n",
    "            \n",
    "            plt.show()\n",
    "            print()\n",
    "\n",
    "\n",
    "# Représentation graphique de la distribution d'une variable pour analyse \n",
    "def representationDistriVaraible(df, key, mesure) :\n",
    "    \n",
    "    #Réprésentation graphique de la distribution de la données\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "    plt.suptitle(f'Répartition des valeurs : {mesure}', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    sns.kdeplot(df[mesure], ax=ax1, fill=True)\n",
    "    \n",
    "    mean, std = np.mean(df[f'{mesure}']), np.std(df[f'{mesure}'])\n",
    "    rvs = np.linspace(mean - 3*std, mean + 3*std, 100)\n",
    "    pdf = norm.pdf(rvs, mean, std)\n",
    "    ax1.plot(rvs, pdf, c=\"r\", label=\"Distribution normale (PDF)\")\n",
    "\n",
    "    ax1.set_title(f'coefficient de skewness : {skew(df[mesure], bias=False):.2f}', fontsize=14)\n",
    "    ax1.set_ylabel('Densité')\n",
    "    ax1.set_xlabel('Valeurs')\n",
    "    ax1.tick_params(labelrotation=45)\n",
    "\n",
    "    df.boxplot(column=mesure, grid=True, ax=ax2)\n",
    "    ax2.set_title('Valeurs extrèmes', fontsize=12)\n",
    "    ax2.set_ylabel('Valeurs')\n",
    "    plt.xticks([1], [''])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Déscription d'une variable pour analyse \n",
    "def varaibleDescribe(df, key, mesure, list_colonne_a_afficher) :\n",
    "    df_value_zero = df.loc[df[mesure] == 0].shape[0]\n",
    "    df_value_not_zero = df.loc[df[mesure] != 0].shape[0]\n",
    "    df_value_nan = df.loc[df[mesure].isna()].shape[0]\n",
    "    \n",
    "    display(pd.DataFrame([[mesure, df.shape[0], df_value_nan, df_value_zero, df_value_not_zero]], columns=['Variable', 'Nb individus', 'Dont null', 'Dont = 0', 'Dont != 0']).set_index('Variable'))\n",
    "    \n",
    "\n",
    "    representationDistriVaraible(df.loc[~df[mesure].isna()], key, mesure)\n",
    "\n",
    "    pd.options.display.float_format = '{:.1f}'.format\n",
    "    df_describe = df[[key, mesure]].describe().T\n",
    "    display(df_describe)\n",
    "\n",
    "    upper = df_describe.iloc[0, 6] + 1.5 * (df_describe.iloc[0, 6] - df_describe.iloc[0, 4])\n",
    "    lower = df_describe.iloc[0, 4] - 1.5 * (df_describe.iloc[0, 6] - df_describe.iloc[0, 4])\n",
    "    \n",
    "    pd.set_option('display.min_rows', 2000)\n",
    "\n",
    "    print()\n",
    "    print(f'  - Extrait de la liste des outilers :')\n",
    "\n",
    "    df_upper_outlier = df.loc[(df[mesure] > upper), [key] + list_colonne_a_afficher + [mesure]].sort_values(by=mesure, ascending=False)\n",
    "    upper_outlier_html = ''\n",
    "    if df_upper_outlier.shape[0] > 0:\n",
    "        upper_outlier_html = df_upper_outlier.head(10).to_html()\n",
    "\n",
    "    df_lower_outlier = df.loc[(df[mesure] < lower), [key] + list_colonne_a_afficher + [mesure]].sort_values(by=mesure, ascending=False)\n",
    "    lower_outlier_html = ''\n",
    "    if df_lower_outlier.shape[0] > 0:\n",
    "        lower_outlier_html = df_lower_outlier.tail(10).to_html()\n",
    "\n",
    "    display(HTML(f'<table><tr><th>{df_lower_outlier.shape[0]} Lower Outliers (< {lower:.2f})</th><th width=\"100\">&nbsp;</th><th>{df_upper_outlier.shape[0]} Upper Outliers (> {upper:.2f})</th></tr><tr><td>{lower_outlier_html}</td><td>&nbsp;</td><td>{upper_outlier_html}</td></tr></table>'))\n",
    "\n",
    "\n",
    "def testNormalisation(df, mesure):\n",
    "    \n",
    "    liste_methode = ['_log', '_sqrt', '_cbrt', '_1x', '_xx']\n",
    "    liste_methode_nom = ['Logrithmique [log(x+1)]', 'Racine carré', 'Racine cubique', 'Inverse [1/(x+1)]', 'Carré']\n",
    "    \n",
    "    coef_skew = skew(df[mesure], bias=False)\n",
    "    \n",
    "    df_courant = df.copy()\n",
    "    df_courant[mesure].fillna(0, inplace=True)\n",
    "    \n",
    "    df_courant[f'{mesure}_log'] = 0\n",
    "    if coef_skew > 0:\n",
    "        df_courant.loc[df_courant[mesure] != 0, [f'{mesure}_log']] = np.log(df[mesure] + 1)\n",
    "    else:\n",
    "        df_courant.loc[df_courant[mesure] != 0, [f'{mesure}_log']] = np.log(max(df[mesure]) + 1 - df[mesure])\n",
    "\n",
    "    df_courant[f'{mesure}_sqrt'] = 0\n",
    "    if coef_skew > 0:\n",
    "        df_courant.loc[df_courant[mesure] != 0, [f'{mesure}_sqrt']] = np.sqrt(df[mesure])\n",
    "    else:\n",
    "        df_courant.loc[df_courant[mesure] != 0, [f'{mesure}_sqrt']] = np.sqrt(max(df[mesure]) + 1 - df[mesure])\n",
    "\n",
    "    df_courant[f'{mesure}_cbrt'] = 0\n",
    "    if coef_skew > 0:\n",
    "        df_courant.loc[df_courant[mesure] != 0, [f'{mesure}_cbrt']] = np.cbrt(df[mesure])\n",
    "    else:\n",
    "        df_courant.loc[df_courant[mesure] != 0, [f'{mesure}_cbrt']] = np.cbrt(max(df[mesure]) + 1 - df[mesure])\n",
    "\n",
    "    df_courant[f'{mesure}_1x'] = 0\n",
    "    if coef_skew > 0:\n",
    "        df_courant.loc[df_courant[mesure] != 0, [f'{mesure}_1x']] = 1 / (df[mesure] + 1)\n",
    "    else:\n",
    "        df_courant.loc[df_courant[mesure] != 0, [f'{mesure}_1x']] = 1 / (max(df[mesure]) + 1 - df[mesure])\n",
    "\n",
    "    df_courant[f'{mesure}_xx'] = df[mesure] * df[mesure]\n",
    "    \n",
    "    #Réprésentation graphique de la distribution de la données\n",
    "    nb_ligne = 2\n",
    "    nb_colonnes = 3\n",
    "    fig, ax = plt.subplots(nrows=nb_ligne, ncols=nb_colonnes, figsize=(18, 12))\n",
    "\n",
    "    plt.suptitle(f'Normalisation de {mesure} - skewness : {coef_skew:.1f}', fontsize=16, y=1.02)\n",
    "    \n",
    "    for methode in liste_methode:\n",
    "        index = liste_methode.index(methode)\n",
    "        \n",
    "        sns.kdeplot(df_courant[f'{mesure}{methode}'], ax=ax[int(index / nb_colonnes), index % nb_colonnes], fill=True)\n",
    "        ax[int(index / nb_colonnes), index % nb_colonnes].set_title(f\"{liste_methode_nom[index]} - skewness : {skew(df_courant[f'{mesure}{methode}'], bias=False):.2f}\", fontsize=14)\n",
    "    \n",
    "        mean, std = np.mean(df_courant[f'{mesure}{methode}']), np.std(df_courant[f'{mesure}{methode}'])\n",
    "        rvs = np.linspace(mean - 3*std, mean + 3*std, 100)\n",
    "        pdf = norm.pdf(rvs, mean, std)\n",
    "        ax[int(index / nb_colonnes), index % nb_colonnes].plot(rvs, pdf, c=\"r\", label=\"Distribution normale (PDF)\")\n",
    "        \n",
    "        ax[int(index / nb_colonnes), index % nb_colonnes].set_ylabel('Densité')\n",
    "        ax[int(index / nb_colonnes), index % nb_colonnes].set_xlabel('Valeurs')\n",
    "        ax[int(index / nb_colonnes), index % nb_colonnes].tick_params(labelrotation=45)\n",
    "        \n",
    "\n",
    "    ax[1, 2].remove()  # don't display empty ax\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return [[mesure,coef_skew, \n",
    "             skew(df_courant[f'{mesure}_log'], bias=False),\n",
    "             skew(df_courant[f'{mesure}_sqrt'], bias=False),\n",
    "             skew(df_courant[f'{mesure}_cbrt'], bias=False),\n",
    "             skew(df_courant[f'{mesure}_1x'], bias=False),\n",
    "             skew(df_courant[f'{mesure}_xx'], bias=False),\n",
    "            ]]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fRepresentationDistributionVariableContinues(df, key, mesure, hue=None, hue_labels=None, list_forcage_varaible_categorielle=None, table_name=None, dict_description_colonne=None):\n",
    "    df = df.copy()\n",
    "    if (mesure != key) & (mesure != hue):\n",
    "        fPrintTitleSouligne(mesure)\n",
    "        if ~((table_name == None) & (dict_description_colonne == None)):\n",
    "            print(dict_description_colonne.get(f\"{table_name} - {mesure}\"))\n",
    "        df_describe = df[mesure].describe()\n",
    "        df_describe = pd.concat([df_describe, pd.DataFrame([f\"{df[mesure].isna().sum()} [{df[mesure].isna().sum()/len(df[mesure]):.2f}%]\"], index=['NaN'])])\n",
    "        #df_describe = df_describe.append(pd.Series([f\"{df[mesure].isna().sum()} [{df[mesure].isna().sum()/len(df[mesure]):.2f}%]\"], index=['NaN']))\n",
    "        df_describe.name = 'Total'\n",
    "        #display(df_describe.T)\n",
    "        if hue:\n",
    "            dff_wip = df[[mesure, hue]].copy()\n",
    "            dff_wip[hue] = dff_wip[hue].apply(lambda x: hue_labels.get(x))\n",
    "#            display(dff_wip)\n",
    "            for var in list(set(dff_wip[hue])):\n",
    "                dff1_wip = dff_wip.loc[(dff_wip[hue]==var), mesure]\n",
    "                dff2_wip = dff1_wip.describe()\n",
    "                dff2_wip = pd.concat([dff2_wip, pd.DataFrame([f\"{dff1_wip.isna().sum()} [{dff1_wip.isna().sum()/len(dff1_wip):.2f}%]\"], index=['NaN'])])\n",
    "                #dff2_wip = dff2_wip.append(pd.Series([f\"{dff1_wip.isna().sum()} [{dff1_wip.isna().sum()/len(dff1_wip):.2f}%]\"], index=['NaN']))\n",
    "                dff2_wip.name = var\n",
    "#                display(dff1_wip)\n",
    "                df_describe = pd.concat([df_describe, dff2_wip], axis=1)\n",
    "                \n",
    "        \n",
    "        display(df_describe.T)\n",
    "        #display(df_4_plot)\n",
    "        print()\n",
    "        print(\"Nombre de valeurs distinctes : \", len(list(set(df[mesure]))))\n",
    "        print()\n",
    "\n",
    "        df = df.loc[~df[mesure].isna(), [key, mesure, hue]].copy()\n",
    "        df[f'{mesure}_num'] = df[mesure]\n",
    "        \n",
    "        if 'timedelta64' not in str(df[mesure].dtypes).lower():\n",
    "            #Réprésentation graphique de la distribution de la données\n",
    "            fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "        \n",
    "            plt.suptitle(f'Répartition des valeurs : {mesure}', fontsize=16, y=1.02)\n",
    "            plt.tight_layout()\n",
    "    \n",
    "            sns.kdeplot(df[mesure], ax=ax1, fill=True, label='Total')\n",
    "            if hue:\n",
    "                dff_wip = df[[mesure, hue]].copy()\n",
    "                dff_wip[hue] = dff_wip[hue].apply(lambda x: hue_labels.get(x))\n",
    "                for var in list(set(dff_wip[hue])):\n",
    "                    dff1_wip = dff_wip.loc[(dff_wip[hue]==var), mesure]\n",
    "                    sns.kdeplot(dff1_wip, ax=ax1, fill=True, label=var)\n",
    "    \n",
    "            if 'time' in str(df[mesure].dtypes).lower():\n",
    "                df[f'{mesure}_num'] = pd.to_datetime(df[mesure]).astype('int64') / 10**9 / 3600 / 24    \n",
    "                \n",
    "            mean, std = np.mean(df[f'{mesure}_num']), np.std(df[f'{mesure}_num'])\n",
    "            rvs = np.linspace(mean - 3*std, mean + 3*std, 100)\n",
    "            pdf = norm.pdf(rvs, mean, std)\n",
    "            ax1.plot(rvs, pdf, c=\"r\", label=\"Distribution normale (PDF)\")\n",
    "            ax1.set_title(f\"coefficient de skewness : {skew(df.loc[~df[mesure].isna(), f'{mesure}_num'], bias=False):.2f}\", fontsize=14)\n",
    "           \n",
    "            ax1.set_ylabel('Densité')\n",
    "                \n",
    "            ax1.set_xlabel('Valeurs')\n",
    "            ax1.legend()\n",
    "            ax1.tick_params(labelrotation=45)\n",
    "        \n",
    "            df.boxplot(column=f'{mesure}_num', grid=True, ax=ax2)\n",
    "            ax2.set_title('Valeurs extrèmes', fontsize=12)\n",
    "            ax2.set_ylabel('Valeurs')\n",
    "            plt.xticks([1], [''])\n",
    "        \n",
    "            plt.show()\n",
    "\n",
    "        df[hue] = df[hue].apply(lambda x: hue_labels.get(x))\n",
    "        print()\n",
    "        display(HTML(f\"<table><tr><td>{pd.DataFrame(df.loc[~df[mesure].isna(), [key, mesure, hue, f'{mesure}_num']].sort_values(by=mesure)).head(15).to_html()}</td><td>{pd.DataFrame(df.loc[~df[mesure].isna(), [key, mesure, hue, f'{mesure}_num']].sort_values(by=mesure)).tail(15).to_html()}</td></tr></table>\"))\n",
    "        print()\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calcul des dimmensions des classes de catégories pour la transformation des variables continues en varaiables catégorielles \n",
    "def getDimensionBins(df, startBins=None) :\n",
    "    if startBins is None :\n",
    "        bins = len(df)\n",
    "    else :\n",
    "        bins = startBins\n",
    "        \n",
    "    try :\n",
    "        pd.qcut(df, bins)\n",
    "    except ValueError:\n",
    "        bins = int(0.75 * bins)\n",
    "        bins = getDimensionBins(df, startBins= bins)\n",
    "    \n",
    "    return bins\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a30770ca960821a6c915f6acd54161a37bffb432"
   },
   "source": [
    "#### Function to Handle Categorical Variables\n",
    "\n",
    "To make the code more efficient, we can now write a function to handle the categorical variables for us. This will take the same form as the `agg_numeric` function in that it accepts a dataframe and a grouping variable. Then it will calculate the counts and normalized counts of each category for all categorical variables in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "ddc7decf9497eaa536255d601f76c3a6259fca6b"
   },
   "outputs": [],
   "source": [
    "def count_categorical(df, group_var, df_name):\n",
    "    \"\"\"Computes counts and normalized counts for each observation\n",
    "    of `group_var` of each unique category in every categorical variable\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "    df : dataframe \n",
    "        The dataframe to calculate the value counts for.\n",
    "        \n",
    "    group_var : string\n",
    "        The variable by which to group the dataframe. For each unique\n",
    "        value of this variable, the final dataframe will have one row\n",
    "        \n",
    "    df_name : string\n",
    "        Variable added to the front of column names to keep track of columns\n",
    "\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    categorical : dataframe\n",
    "        A dataframe with counts and normalized counts of each unique category in every categorical variable\n",
    "        with one row for every unique value of the `group_var`.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the categorical columns\n",
    "    categorical = one_hot_encoder(df.select_dtypes('object'))\n",
    "\n",
    "    # Make sure to put the identifying id on the column\n",
    "    categorical[group_var] = df[group_var]\n",
    "\n",
    "    # Groupby the group var and calculate the sum and mean\n",
    "    categorical = categorical.groupby(group_var).agg(['sum', 'mean'])\n",
    "    \n",
    "    column_names = []\n",
    "    \n",
    "    # Iterate through the columns in level 0\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        # Iterate through the stats in level 1\n",
    "        for stat in ['count', 'count_norm']:\n",
    "            # Make a new column name\n",
    "            column_names.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    \n",
    "    categorical.columns = column_names\n",
    "    \n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "ddc7decf9497eaa536255d601f76c3a6259fca6b"
   },
   "outputs": [],
   "source": [
    "# Fonction pour ré\"cupérer la liste exhaustive des valeurs d'un dataframe\n",
    "def fGetDictOfCategorialValues(df):\n",
    "    dict_of_values = {}\n",
    "    for var in [k for k, v in df.dtypes.to_dict().items() if str(v) == 'object']:\n",
    "        dict_of_values[var] = list(set(df[var]))\n",
    "\n",
    "    return dict_of_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fSetBoolEncoding(df, list_of_col_to_encode, values_to_encode=None):\n",
    "    \n",
    "    # Create a label encoder object\n",
    "    le = LabelEncoder()\n",
    "    le_count = 0\n",
    "    \n",
    "    # Iterate through the columns\n",
    "    for col in list_of_col_to_encode:\n",
    "        if values_to_encode:\n",
    "            le.fit(values_to_encode)\n",
    "            df[col] = le.transform(df[col])\n",
    "\n",
    "        df[col] = df[col].astype('bool')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ddc7decf9497eaa536255d601f76c3a6259fca6b"
   },
   "outputs": [],
   "source": [
    "def fSetEncodingCategorialFeatures(df, dict_of_values, label_encoding_uniquement=False):\n",
    "    \n",
    "    # Create a label encoder object\n",
    "    le = LabelEncoder()\n",
    "    le_count = 0\n",
    "    \n",
    "    # Iterate through the columns\n",
    "    for col in df:\n",
    "        if df[col].dtype == 'object':\n",
    "            # If 2 or fewer unique categories\n",
    "            if len(dict_of_values.get(col)) <= 2:\n",
    "                # Train on possible values\n",
    "                le.fit(dict_of_values.get(col))\n",
    "                # Transform both training and testing data\n",
    "                df[col] = le.transform(df[col])\n",
    "                \n",
    "                # Keep track of how many columns were label encoded\n",
    "                le_count += 1\n",
    "                \n",
    "    print('%d colonnes sont traité par Label Encoding.' % le_count)\n",
    "    if label_encoding_uniquement == False:\n",
    "        # one-hot encoding of categorical variables\n",
    "        df, new_columns = one_hot_encoder(df)\n",
    "\n",
    "    return_colonnes = [x for x in list(df) if 'Data_Not_Available' not in x]\n",
    "    df = df[return_colonnes]\n",
    "    \n",
    "    print('Features shape: ', df.shape)\n",
    "    if label_encoding_uniquement == False:\n",
    "        print(\"liste des nouvelle colonnes : \", [x for x in return_colonnes if x in new_columns])\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11f1b0ab2146ec95a50055d2788b5f05c43e0afb"
   },
   "source": [
    "#### Aggregating Numeric Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_numeric(df, group_var, df_name):\n",
    "    \"\"\"Aggregates the numeric values in a dataframe. This can\n",
    "    be used to create features for each instance of the grouping variable.\n",
    "    \n",
    "    Parameters\n",
    "    --------\n",
    "        df (dataframe): \n",
    "            the dataframe to calculate the statistics on\n",
    "        group_var (string): \n",
    "            the variable by which to group df\n",
    "        df_name (string): \n",
    "            the variable used to rename the columns\n",
    "        \n",
    "    Return\n",
    "    --------\n",
    "        agg (dataframe): \n",
    "            a dataframe with the statistics aggregated for \n",
    "            all numeric columns. Each instance of the grouping variable will have \n",
    "            the statistics (mean, min, max, sum; currently supported) calculated. \n",
    "            The columns are also renamed to keep track of features created.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Remove id variables other than grouping variable\n",
    "    for col in df:\n",
    "        if col != group_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns = col)\n",
    "            \n",
    "    group_ids = df[group_var]\n",
    "    numeric_df = df.select_dtypes('number')\n",
    "    numeric_df[group_var] = group_ids\n",
    "\n",
    "    # Group by the specified variable and calculate the statistics\n",
    "    agg = numeric_df.groupby(group_var).agg(['count', 'mean', 'max', 'min', 'sum', 'std']).reset_index()\n",
    "\n",
    "    # Need to create new column names\n",
    "    columns = [group_var]\n",
    "\n",
    "    # Iterate through the variables names\n",
    "    for var in agg.columns.levels[0]:\n",
    "        # Skip the grouping variable\n",
    "        if var != group_var:\n",
    "            # Iterate through the stat names\n",
    "            for stat in agg.columns.levels[1][:-1]:\n",
    "                # Make a new column name for the variable and stat\n",
    "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
    "\n",
    "    agg.columns = columns\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse des traitements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fGetExecuteTime(start_time, decalage=0):\n",
    "    if start_time:\n",
    "        duration = float(datetime.now().timestamp() - start_time)\n",
    "        s_duration = f\"{str(timedelta(seconds=duration))[:-5]}\"\n",
    "        fPrintTitle(f\"\\n{datetime.now().strftime('%d/%m/%Y, %H:%M:%S')}, durée d'exécution : {s_duration}\", decalage=decalage)\n",
    "    \n",
    "    return datetime.now().timestamp()\n",
    "\n",
    "start_time = fGetExecuteTime(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fSplitDataSetForModelingTesting(df, target_label='TARGET', train_target_values=[0, 1], ratio_sampling=1.):\n",
    "    train = df.loc[df[target_label].isin(train_target_values)]\n",
    "    target = train[target_label]\n",
    "    print(f\"Train shape : {train.shape}, target répartition : {Counter(target)}.\")\n",
    "    train.drop(columns=target_label, inplace=True)\n",
    "    train.reset_index(drop=True)\n",
    "    \n",
    "    test = df.loc[~df[target_label].isin(train_target_values)]\n",
    "    test.drop(columns=target_label, inplace=True)\n",
    "    test.reset_index(drop=True)\n",
    "    print(f\"Test shape : {test.shape}.\")\n",
    "    print()\n",
    "    \n",
    "    if ratio_sampling < 1.:\n",
    "        print(f\"Sampling ratio : {100 * ratio_sampling}%\")\n",
    "        train = pd.DataFrame([])\n",
    "        for value in train_target_values:\n",
    "            train = pd.concat([train, \n",
    "                               df.loc[df[target_label] == value].sample(n=int(df.loc[df[target_label] == value].shape[0] * ratio_sampling), random_state=72)\n",
    "                              ])\n",
    "        target = train[target_label]\n",
    "        print(f\"Train sample shape : {train.shape}, Sample target répartition : {Counter(target)}.\")\n",
    "        train.drop(columns=target_label, inplace=True)\n",
    "        train.reset_index(drop=True)\n",
    "\n",
    "        test = df.loc[~df[target_label].isin(train_target_values)].sample(n=int(df.loc[~df[target_label].isin(train_target_values)].shape[0] * ratio_sampling), random_state=72)\n",
    "        test.drop(columns=target_label, inplace=True)\n",
    "        test.reset_index(drop=True)\n",
    "        print(f\"Test sample shape : {test.shape}.\")\n",
    "        print()\n",
    "\n",
    "    return train, target, test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fPlotResults(df_score, type_model=None, max_values=100):\n",
    "    nb_test = df_score.shape[0]\n",
    "\n",
    "    df_4_plot = df_score.sort_values(by=['Equilibrage', 'Modèle', 'Scaler', 'Score'], ascending=[True, True, True, False]).copy()\n",
    "    df_4_plot = df_4_plot.loc[df_score['Score'] < 1.0].drop_duplicates(subset=['Equilibrage', 'Modèle', 'Scaler'])\n",
    "    df_4_plot = df_4_plot.sort_values(by='Score', ascending=False).reset_index(drop=True).copy()\n",
    "    \n",
    "    if type_model:\n",
    "        df_4_plot = df_4_plot.loc[df_4_plot['Modèle'] == type_model]\n",
    "\n",
    "    features_to_keep = []\n",
    "    for features_out in df_4_plot['Liste features out']:\n",
    "        features_to_keep = list(sorted(set(features_to_keep + list(features_out))))\n",
    "    \n",
    "    print(f\"Liste des {len(features_to_keep)} features les plus pertinentes:\\n\", features_to_keep)\n",
    "    print()\n",
    "    \n",
    "    if max_values == None:\n",
    "        max_values = nb_test\n",
    "\n",
    "    df_4_plot = df_4_plot.head(max_values)\n",
    "    \n",
    "    df_4_plot.loc[df_4_plot['Scaler'].isna(), 'Scaler'] = \"No Scaler\"\n",
    "    df_4_plot['Modèle'] = df_4_plot['Modèle'] + \" - \" + df_4_plot['Scaler']\n",
    "    df_4_plot.drop(columns='Scaler', inplace=True)\n",
    "    \n",
    "    df_4_plot['Score'] = df_4_plot['Score'].apply(lambda x: 100 * x)\n",
    "    \n",
    "    display(df_4_plot[['Equilibrage', 'Modèle', 'Score', 'Ecat type', 'Itération', 'Nb features in', 'Nb features out']].head())\n",
    "    \n",
    "    df_4_plot = df_4_plot[['Equilibrage', 'Modèle', 'Score']].pivot(index = 'Modèle', columns = 'Equilibrage', values = 'Score')\n",
    "    \n",
    "    df_4_plot.sort_values(by='Modèle', inplace=True, key=lambda col: col.str.lower()) \n",
    "    \n",
    "    ## Représentation graphique\n",
    "        \n",
    "    fig = plt.figure(figsize=(df_4_plot.shape[1] + 2, df_4_plot.shape[0] + 2))    \n",
    "    \n",
    "    sns.heatmap(df_4_plot, annot=True, square=True, \n",
    "                            fmt='.1f', cmap=\"Blues\", cbar=False, annot_kws={\"fontsize\":10}) #, yticklabels=targets_labels)\n",
    "\n",
    "    if max_values < nb_test:\n",
    "        title_detail = f\"{max_values} meilleurs scores/{nb_test} tests\"\n",
    "    else:\n",
    "        title_detail = f\"{nb_test} tests\"\n",
    "\n",
    "    plt.title(f\"Score of the model ({title_detail})\", fontsize = 18, fontweight = 'bold')\n",
    "    plt.ylabel('Modèle', fontsize = 14, fontweight = 'bold')\n",
    "    plt.xlabel('DataSet - Equilibrage des jeux de données', fontsize = 14, fontweight = 'bold')\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fGetClassWeight(target):\n",
    "    dict_class_weight = {}\n",
    "    n_samples = len(target)\n",
    "    n_classes = len(list(set(target)))\n",
    "    c = Counter(target)\n",
    "    for value in list(set(target)):\n",
    "        dict_class_weight[value] = n_samples / (n_classes * c[value])\n",
    "    \n",
    "    return dict_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "a90e9368-5f7d-4179-a5cc-1025f32c6a81",
    "_uuid": "b912337a5f35f495398d8ae8b8576ceb7062fe50"
   },
   "outputs": [],
   "source": [
    " def plot_feature_importances(df):\n",
    "    \"\"\"\n",
    "    Plot importances returned by a model. This can work with any measure of\n",
    "    feature importance provided that higher importance is better. \n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): feature importances. Must have the features in a column\n",
    "        called `features` and the importances in a column called `importance\n",
    "        \n",
    "    Returns:\n",
    "        shows a plot of the 15 most importance features\n",
    "        \n",
    "        df (dataframe): feature importances sorted by importance (highest to lowest) \n",
    "        with a column for normalized importance\n",
    "        \"\"\"\n",
    "    \n",
    "    # Sort features according to importance\n",
    "    df = df.sort_values('importance', ascending = False).reset_index()\n",
    "    \n",
    "    # Normalize the feature importances to add up to one\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "\n",
    "    # Make a horizontal bar chart of feature importances\n",
    "    plt.figure(figsize = (10, 5))\n",
    "    ax = plt.subplot()\n",
    "    \n",
    "    # Need to reverse the index to plot most important on top\n",
    "    ax.barh(list(reversed(list(df.index[:15]))), \n",
    "            df['importance_normalized'].head(15), \n",
    "            align = 'center', edgecolor = 'k')\n",
    "    \n",
    "    # Set the yticks and labels\n",
    "    ax.set_yticks(list(reversed(list(df.index[:15]))))\n",
    "    ax.set_yticklabels(df['feature'].head(15))\n",
    "    \n",
    "    # Plot labeling\n",
    "    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scoring(y_true, y_pred, beta=None):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    if beta == None :\n",
    "        c = Counter(y_true)\n",
    "        if c[0] > c[1]:\n",
    "            beta = c[1] > c[0]\n",
    "        else:\n",
    "            beta = c[0] > c[1]\n",
    "        \n",
    "    fb = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "    return fb\n",
    "\n",
    "def custom_scoring_min_fp(y_true, y_pred):\n",
    "    # Pas d'accord de prêt alors qu'ils n'auraient pas fait de défaut de paiement\n",
    "    return custom_scoring(y_true, y_pred, beta=0.5)\n",
    "\n",
    "def custom_scoring_ratio_classweight(y_true, y_pred):\n",
    "    return custom_scoring(y_true, y_pred, beta=None)\n",
    "\n",
    "def custom_scoring_min_fn(y_true, y_pred):\n",
    "    # Accord de prêt alors qu'ils vont être en défaut de paiement\n",
    "    return custom_scoring(y_true, y_pred, beta=2)\n",
    "\n",
    "def custom_fbeta_score(y_true, y_pred):\n",
    "    # Accord de prêt alors qu'ils vont être en défaut de paiement\n",
    "    return fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "custom_scorer = make_scorer(custom_scoring_min_fn, greater_is_better=True, needs_proba=False)\n",
    "\n",
    "custom_f2_scorer = make_scorer(custom_fbeta_score, greater_is_better=True, needs_proba=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Vérification des versions des librairies Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python :\n",
      "    - version utilisée   : 3.11.2\n",
      "    - version disponible : 3.11.5\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Version notebook</th>\n",
       "      <th>Nouvelle version disponible sur pypi.org</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Package</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dummy</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Featuretools</th>\n",
       "      <td>1.27.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ipython</th>\n",
       "      <td>8.14.0</td>\n",
       "      <td>8.15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ipywidgets</th>\n",
       "      <td>8.1.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lightgbm</th>\n",
       "      <td>4.0.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lxml</th>\n",
       "      <td>4.9.3</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Matplotlib</th>\n",
       "      <td>3.7.2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missingno</th>\n",
       "      <td>0.5.2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Numpy</th>\n",
       "      <td>1.24.4</td>\n",
       "      <td>1.25.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pandas</th>\n",
       "      <td>2.0.3</td>\n",
       "      <td>2.1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Requests</th>\n",
       "      <td>2.31.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scipy</th>\n",
       "      <td>1.11.2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Seaborn</th>\n",
       "      <td>0.12.2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sklearn</th>\n",
       "      <td>1.2.2</td>\n",
       "      <td>1.3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Xgboost</th>\n",
       "      <td>1.7.5</td>\n",
       "      <td>1.7.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Version notebook Nouvelle version disponible sur pypi.org\n",
       "Package                                                               \n",
       " Dummy                    1.0                                      1.1\n",
       "Featuretools           1.27.0                                         \n",
       "Ipython                8.14.0                                   8.15.0\n",
       "Ipywidgets              8.1.0                                         \n",
       "Lightgbm                4.0.0                                         \n",
       "Lxml                    4.9.3                                         \n",
       "Matplotlib              3.7.2                                         \n",
       "Missingno               0.5.2                                         \n",
       "Numpy                  1.24.4                                   1.25.2\n",
       "Pandas                  2.0.3                                    2.1.0\n",
       "Requests               2.31.0                                         \n",
       "Scipy                  1.11.2                                         \n",
       "Seaborn                0.12.2                                         \n",
       "Sklearn                 1.2.2                                    1.3.0\n",
       "Xgboost                 1.7.5                                    1.7.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Liste des packages non inclus dans Python a vérifier\n",
    "package = [\n",
    "    ft,\n",
    "    ipw, \n",
    "    ipy,\n",
    "    lgb,\n",
    "    lxml, \n",
    "    mpl, \n",
    "    msno, \n",
    "    np, \n",
    "    pd,\n",
    "    req, \n",
    "    sci,\n",
    "    skl, \n",
    "    sns,\n",
    "    xgb\n",
    "]\n",
    "\n",
    "print(f\"Python :\")\n",
    "print(f\"    - version utilisée   : {sys.version.split(' ')[0]}\")\n",
    "print(f\"    - version disponible : {getCurrentVersionPython().replace('_', ' ')}\")\n",
    "print()\n",
    "\n",
    "liste_version_package = [[f' {dummy.__name__.capitalize()}', dummy.__version__, getCurrentVersionPipProject('data-dummy').split(' ')[-1:][0].replace('_', ' '), '']]\n",
    "\n",
    "for lib in package:\n",
    "    if lib != '':\n",
    "        try:\n",
    "            liste_version_package.append([lib.__name__.capitalize(), lib.__version__.split('+')[0], getCurrentVersionPipProject(lib.__name__).split(' ')[-1:][0].replace('_', ' '), ''])\n",
    "        except:\n",
    "            print(lib)\n",
    "\n",
    "df_package = pd.DataFrame(liste_version_package, columns=['Package', 'Version notebook', 'Version pypi.org', 'Nouvelle version disponible sur pypi.org']).set_index('Package')\n",
    "try:\n",
    "    df_package.loc[df_package['Version notebook'] != df_package['Version pypi.org'], 'Nouvelle version disponible sur pypi.org'] = df_package['Version pypi.org']\n",
    "except:\n",
    "    print('Doublon de package !')\n",
    "    display(df_package)\n",
    "\n",
    "display(df_package.drop(columns='Version pypi.org').sort_values('Package'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durée partielle des traitements à 05/09/2023, 12:20:31, durée : 0:00:16.978276\n"
     ]
    }
   ],
   "source": [
    "endtime = datetime.now()\n",
    "\n",
    "#print(f'Fin des traitements à {endtime.strftime(\"%d/%m/%Y, %H:%M:%S\")}, durée : {int(str(endtime - startime).split(\":\")[0]):02d}h {int(str(endtime - startime).split(\":\")[1]):02d}min')\n",
    "print(f'Durée partielle des traitements à {endtime.strftime(\"%d/%m/%Y, %H:%M:%S\")}, durée : {endtime - startime}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables d'environnements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Définition des options d'affichages des tableaux de données\n",
    "pd.set_option('display.min_rows', 2000)\n",
    "pd.set_option('display.max_columns', 2000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "pd.options.display.float_format = '{:f}'.format\n",
    "\n",
    "# For Windows\n",
    "#set HTTP_PROXY=http://USER:PWD@proxy.company.com:PORT\n",
    "#set HTTPS_PROXY=https://USER:PWD@proxy.company.com:PORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\petx698\\OneDrive - LA POSTE GROUPE\\Documents\\MyDev\\Python\\Projets\\ocr\\ocr-ds-projet7\n"
     ]
    }
   ],
   "source": [
    "# Répertoire courrant\n",
    "print(os.getcwd())\n",
    "\n",
    "# Dossier contenant les fichiers de données\n",
    "DATA_DIR_SOURCE = os.path.join(\"data\", \"source\")\n",
    "DATA_DIR_CLEANED = os.path.join(\"data\", \"cleaned\")\n",
    "DATA_DIR_AUTRES = os.path.join(\"data\", \"autres\")\n",
    "\n",
    "full_dataset = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb CPU physique :  4\n",
      "Nb CPU logique  :  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Nb CPU physique : \", psutil.cpu_count(logical=False))\n",
    "print(\"Nb CPU logique  : \", psutil.cpu_count(logical=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init sns\n",
    "sns.set_style(\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "26848bd0252a31559e254c454b905305701ee522"
   },
   "outputs": [],
   "source": [
    "# Free up memory by deleting old objects\n",
    "#import gc\n",
    "gc.enable()\n",
    "#del train, bureau, bureau_balance, bureau_agg, bureau_agg_new, bureau_balance_agg, bureau_balance_counts, bureau_by_loan, bureau_balance_by_client, bureau_counts\n",
    "#gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
